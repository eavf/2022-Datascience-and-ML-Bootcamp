{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From F:\\GITHUB\\2022 Datascience and ML Bootcamp\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(888)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(404)\n",
    "\n",
    "from tensorflow.summary import create_file_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from time import strftime\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN_PATH = 'MNIST/digit_xtrain.csv'\n",
    "X_TEST_PATH = 'MNIST/digit_xtest.csv'\n",
    "Y_TRAIN_PATH = 'MNIST/digit_ytrain.csv'\n",
    "Y_TEST_PATH = 'MNIST/digit_ytest.csv'\n",
    "\n",
    "LOGGING_PATH = 'tensorboard_mnist_digit_logs/'\n",
    "\n",
    "NR_CLASSES = 10\n",
    "VALIDATION_SIZE = 10000\n",
    "IMAGE_WIDTH = 28\n",
    "IMAGE_HEIGHT = 28\n",
    "CHANNELS = 1\n",
    "TOTAL_INPUTS = IMAGE_WIDTH*IMAGE_HEIGHT*CHANNELS\n",
    "\n",
    "\n",
    "nr_epochs = 70\n",
    "learning_rate = 5e-4\n",
    "\n",
    "n_hidden1 = 512\n",
    "n_hidden2 = 64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_train_all = np.loadtxt(Y_TRAIN_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.loadtxt(Y_TEST_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5.86 s\n",
      "Wall time: 5.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "x_train_all = np.loadtxt(X_TRAIN_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "x_test = np.loadtxt(X_TEST_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_all[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-scale features - aby sme dostali hodnoty 0 - 1 a nie 0 - 255, ako je to teraz\n",
    "def preprocess_data(x, y):\n",
    "    # Normalize images\n",
    "    x = x / 255.0\n",
    "\n",
    "    # One-hot encode labels\n",
    "    y = tf.one_hot(y.astype(np.int32), depth=NR_CLASSES)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    # Load data from files\n",
    "    x_train_all = np.loadtxt(X_TRAIN_PATH, delimiter=',', dtype=np.float32)\n",
    "    y_train_all = np.loadtxt(Y_TRAIN_PATH, delimiter=',', dtype=np.int32)\n",
    "    x_test = np.loadtxt(X_TEST_PATH, delimiter=',', dtype=np.float32)\n",
    "    y_test = np.loadtxt(Y_TEST_PATH, delimiter=',', dtype=np.int32)\n",
    "    \n",
    "    # x -> sú obrázky v numpy\n",
    "    # y -> sú správne čísla na nich\n",
    "\n",
    "    # Validation dataset;\n",
    "    x_val = x_train_all[:VALIDATION_SIZE]\n",
    "    y_val = y_train_all[:VALIDATION_SIZE]\n",
    "\n",
    "    x_train = x_train_all[VALIDATION_SIZE:]\n",
    "    y_train = y_train_all[VALIDATION_SIZE:]\n",
    "\n",
    "    # Preprocess the data\n",
    "    x_train, y_train = preprocess_data(x_train, y_train)\n",
    "    x_val, y_val = preprocess_data(x_val, y_val)\n",
    "    x_test, y_test = preprocess_data(x_test, y_test)\n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tu reteštartovať"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_val, y_val, x_test, y_test = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Setup Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder for Tensorboard\n",
    "folder_name = f'Model 1 at {strftime(\"%m %d %Y - %H %M\")}'\n",
    "directory = os.path.join(LOGGING_PATH, folder_name)\n",
    "\n",
    "try:\n",
    "    os.makedirs(directory)\n",
    "except OSError as exception:\n",
    "    print(exception.strerror)\n",
    "else:\n",
    "    print('Succesfully created directory')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up writers ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writer definition\n",
    "train_writer = create_file_writer(directory + '/train')\n",
    "val_writer = create_file_writer(directory + '/val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "# Model without dropout\n",
    "class HandwritingModel(tf.Module):\n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, output_size):\n",
    "        super(HandwritingModel, self).__init__()\n",
    "        # Initialize weights and biases for the first hidden layer\n",
    "        self.w1 = tf.Variable(tf.random.truncated_normal([input_size, hidden1_size], stddev=0.1))\n",
    "        self.b1 = tf.Variable(tf.constant(0.1, shape=[hidden1_size]))\n",
    "\n",
    "        # Initialize weights and biases for the second hidden layer\n",
    "        self.w2 = tf.Variable(tf.random.truncated_normal([hidden1_size, hidden2_size], stddev=0.1))\n",
    "        self.b2 = tf.Variable(tf.constant(0.1, shape=[hidden2_size]))\n",
    "\n",
    "        # Initialize weights and biases for the output layer\n",
    "        self.w3 = tf.Variable(tf.random.truncated_normal([hidden2_size, output_size], stddev=0.1))\n",
    "        self.b3 = tf.Variable(tf.constant(0.1, shape=[output_size]))\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, TOTAL_INPUTS], dtype=tf.float32)])\n",
    "    def __call__(self, x):\n",
    "        # First hidden layer with ReLU activation\n",
    "        with tf.name_scope('hidden_layer_1'):\n",
    "            z1 = tf.matmul(x, self.w1) + self.b1\n",
    "            a1 = tf.nn.relu(z1)\n",
    "\n",
    "        # Second hidden layer with ReLU activation\n",
    "        with tf.name_scope('hidden_layer_2'):\n",
    "            z2 = tf.matmul(a1, self.w2) + self.b2\n",
    "            a2 = tf.nn.relu(z2)\n",
    "\n",
    "        # Output layer with linear activation\n",
    "        with tf.name_scope('output_layer'):\n",
    "            z3 = tf.matmul(a2, self.w3) + self.b3\n",
    "        return z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = HandwritingModel(TOTAL_INPUTS, n_hidden1, n_hidden2, NR_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Model definition\n",
    "# Model with dropout\n",
    "class HandwritingModel_drop(tf.Module):\n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, output_size):\n",
    "        super(HandwritingModel_drop, self).__init__()\n",
    "        # Initialize weights and biases for the first hidden layer\n",
    "        self.w1 = tf.Variable(tf.random.truncated_normal([input_size, hidden1_size], stddev=0.1))\n",
    "        self.b1 = tf.Variable(tf.constant(0.1, shape=[hidden1_size]))\n",
    "\n",
    "        # Initialize weights and biases for the second hidden layer\n",
    "        self.w2 = tf.Variable(tf.random.truncated_normal([hidden1_size, hidden2_size], stddev=0.1))\n",
    "        self.b2 = tf.Variable(tf.constant(0.1, shape=[hidden2_size]))\n",
    "\n",
    "        # Initialize weights and biases for the output layer\n",
    "        self.w3 = tf.Variable(tf.random.truncated_normal([hidden2_size, output_size], stddev=0.1))\n",
    "        self.b3 = tf.Variable(tf.constant(0.1, shape=[output_size]))\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, TOTAL_INPUTS], dtype=tf.float32)])\n",
    "    def __call__(self, x):\n",
    "        # First hidden layer with ReLU activation\n",
    "        with tf.name_scope('hidden_layer_1'):\n",
    "            z1 = tf.matmul(x, self.w1) + self.b1\n",
    "            a1 = tf.nn.relu(z1)\n",
    "\n",
    "        # Drop hidden layer\n",
    "        with tf.name_scope('drop_layer'):\n",
    "            dz1 = tf.nn.dropout(a1, rate=0.8, name='dropout_layer')\n",
    "\n",
    "        # Second hidden layer with ReLU activation\n",
    "        with tf.name_scope('hidden_layer_2'):\n",
    "            z2 = tf.matmul(dz1, self.w2) + self.b2\n",
    "            a2 = tf.nn.relu(z2)\n",
    "\n",
    "        # Output layer with linear activation\n",
    "        with tf.name_scope('output_layer'):\n",
    "            z3 = tf.matmul(a2, self.w3) + self.b3\n",
    "        return z3\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss, Optimisation & Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits, labels):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimiser = tf.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Accuracy Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a accuracy metric\n",
    "def accuracy_fn(predictions, labels):\n",
    "    correct_predictions = tf.equal(tf.argmax(predictions, 1), tf.argmax(labels, 1))\n",
    "    return tf.reduce_mean(tf.cast(correct_predictions, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Input Images in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with tf.name_scope('show_image'):\n",
    "#    x_image = tf.reshape(X, [-1, 28, 28, 1])\n",
    "#    tf.summary.image('image_input', x_image, max_outputs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up učiacej slučky\n",
    "size_of_batch = 1000\n",
    "num_examples = y_train.shape[0]\n",
    "print(num_examples)\n",
    "nr_iterations = int(num_examples/size_of_batch)\n",
    "print(nr_iterations)\n",
    "index_in_epoch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Filewriter and Merge Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nakreslenie tf grafu pre učiacu slučku .....\n",
    "def pisac(model, x = \"WDO\"):\n",
    "    # Get the concrete function\n",
    "    concrete_function = model.__call__.get_concrete_function()\n",
    "\n",
    "    # Create a summary file writer\n",
    "    if x == \"DO\":\n",
    "        writer = tf.summary.create_file_writer(directoryDO)\n",
    "    else:\n",
    "        writer = tf.summary.create_file_writer(directory)\n",
    "\n",
    "    # Use the writer to write the graph\n",
    "    with writer.as_default():\n",
    "        tf.summary.graph(concrete_function.graph)\n",
    "        writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcia vráti batch z dát......\n",
    "def get_batches(batch_size, x, y):\n",
    "    for start in range(0, len(x), batch_size):\n",
    "        end = start + batch_size\n",
    "        yield x[start:end], y[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop - Funkcia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nauc (model):\n",
    "    for epoch in range(nr_epochs):\n",
    "        total_accuracy = tf.constant(0, dtype=tf.float32)\n",
    "        num_batches = tf.constant(0, dtype=tf.float32)\n",
    "\n",
    "        for batch_x, batch_y in get_batches(size_of_batch, x_train, y_train):\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Vypočítame predpoved\n",
    "                logits = model(batch_x)\n",
    "                # Stratu\n",
    "                loss = loss_fn(logits, batch_y)\n",
    "                # úspešnosť\n",
    "                accuracy = accuracy_fn(logits, batch_y)\n",
    "                total_accuracy += accuracy\n",
    "\n",
    "        # Compute gradients\n",
    "        gradients = tape.gradient(loss, [model.w1, model.b1, model.w2, model.b2, model.w3, model.b3])\n",
    "        if any(g is None for g in gradients):\n",
    "            print(gradients)\n",
    "            raise ValueError(\"One or more gradients are None\")\n",
    "            \n",
    "        # Apply gradients\n",
    "        optimiser.apply_gradients(zip(gradients, [model.w1, model.b1, model.w2, model.b2, model.w3, model.b3]))\n",
    "        \n",
    "        # Average accuracy\n",
    "        average_train_accuracy = total_accuracy / num_batches\n",
    "            \n",
    "        with train_writer.as_default():\n",
    "            for var, varname, grad in zip([model.w1, model.b1, model.w2, model.b2, model.w3, model.b3], ['w1', 'b1', 'w2', 'b2', 'w3', 'b3'], gradients):\n",
    "                tf.summary.histogram(varname, var, step=epoch)\n",
    "                tf.summary.histogram(f\"{varname}/grad\", grad, step=epoch)\n",
    "            # Performance metrics\n",
    "            with tf.name_scope('performance'):\n",
    "                tf.summary.scalar('accuracy', accuracy, step=epoch)\n",
    "                tf.summary.scalar('cost', loss, step=epoch)\n",
    "\n",
    "        num_batches += 1\n",
    "\n",
    "        # Validation\n",
    "        val_logits = model(tf.cast(x_val, tf.float32))\n",
    "        val_loss = loss_fn(val_logits, y_val)\n",
    "        val_accuracy = accuracy_fn(val_logits, y_val)\n",
    "        \n",
    "        with val_writer.as_default():\n",
    "            with tf.name_scope('performance'):\n",
    "                tf.summary.scalar('accuracy', val_accuracy, step=epoch)\n",
    "                tf.summary.scalar('cost', val_loss, step=epoch)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}, Loss (validation): {val_loss.numpy()}, Training Accuracy: {average_train_accuracy.numpy() * 100:.20f}%, Validation Accuracy: {val_accuracy.numpy() * 100:.2f}%\")\n",
    "\n",
    "        train_writer.flush()\n",
    "        val_writer.flush()\n",
    "\n",
    "    # Napíše graf modelu......\n",
    "    pisac(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A teraz natrenovanie..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model bez Dropoff\n",
    "nauc(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('MNIST/test_img.png')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konverzia do ČB obrázka\n",
    "bw = img.convert('L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = np.invert(bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = img_array.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the test_img to match the expected shape (None, 784)\n",
    "test_img = test_img.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tf.argmax(mod(test_img), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Prediction for test image is {predictions}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teraz všetky obrázky....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_directory = 'MNIST/'\n",
    "processed_images = []\n",
    "preds = []\n",
    "\n",
    "def tfprocimg(obr):\n",
    "    img = Image.open(obr)\n",
    "    bw = img.convert('L')\n",
    "    img_array = np.invert(bw)\n",
    "    test_img = img_array.ravel()\n",
    "    test_img = test_img.reshape(1, -1)\n",
    "    return test_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(image_directory):\n",
    "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "        # Check if the file is a JPEG or PNG image (you can add more formats as needed)\n",
    "        file_path = os.path.join(image_directory, filename)\n",
    "        \n",
    "        try:\n",
    "            # Open and process the image using Pillow\n",
    "            ts_im = tfprocimg(file_path)\n",
    "            predict = tf.argmax(mod(ts_im), axis=1)\n",
    "            print(f'File: {file_path} is number {predict}')\n",
    "            \n",
    "            # Append the processed image to the list and preds\n",
    "            processed_images.append(file_path)\n",
    "            preds.append(predict)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge:** Calculate the accuracy over the test dataset (```x_test``` and ```y_test```). Use your knowledge of running a session to get the accuracy. Display the accuracy as a percentage rounded to two decimal numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = mod(x_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(predictions, axis=1), tf.argmax(y_test, axis=1)), tf.float32))\n",
    "\n",
    "# Convert accuracy to a percentage and round to two decimal places\n",
    "accuracy_percent = tf.round(test_accuracy * 100, 2)\n",
    "\n",
    "# Print the accuracy as a percentage\n",
    "print(f\"Accuracy: {accuracy_percent}%\")\n",
    "                          \n",
    "print(f'Accuracy on test set is {test_accuracy:0.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Reset for the Next Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer.close()\n",
    "val_writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
