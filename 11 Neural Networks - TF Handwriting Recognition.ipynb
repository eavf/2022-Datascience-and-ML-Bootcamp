{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From F:\\GITHUB\\2022 Datascience and ML Bootcamp\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(888)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(404)\n",
    "\n",
    "from tensorflow.summary import create_file_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from time import strftime\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN_PATH = 'MNIST/digit_xtrain.csv'\n",
    "X_TEST_PATH = 'MNIST/digit_xtest.csv'\n",
    "Y_TRAIN_PATH = 'MNIST/digit_ytrain.csv'\n",
    "Y_TEST_PATH = 'MNIST/digit_ytest.csv'\n",
    "\n",
    "LOGGING_PATH = 'tensorboard_mnist_digit_logs/'\n",
    "\n",
    "NR_CLASSES = 10\n",
    "VALIDATION_SIZE = 10000\n",
    "IMAGE_WIDTH = 28\n",
    "IMAGE_HEIGHT = 28\n",
    "CHANNELS = 1\n",
    "TOTAL_INPUTS = IMAGE_WIDTH*IMAGE_HEIGHT*CHANNELS\n",
    "\n",
    "\n",
    "nr_epochs = 70\n",
    "learning_rate = 5e-4\n",
    "\n",
    "n_hidden1 = 512\n",
    "n_hidden2 = 64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 12 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_train_all = np.loadtxt(Y_TRAIN_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.loadtxt(Y_TEST_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5.86 s\n",
      "Wall time: 5.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "x_train_all = np.loadtxt(X_TRAIN_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.06 s\n",
      "Wall time: 1.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "x_test = np.loadtxt(X_TEST_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,  18,  18,\n",
       "       126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "       253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253,\n",
       "       253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 219, 253,\n",
       "       253, 253, 253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        80, 156, 107, 253, 253, 205,  11,   0,  43, 154,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,  14,   1, 154, 253,  90,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 139, 253, 190,   2,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,  70,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "       241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,  45, 186, 253, 253, 150,  27,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  16,  93, 252, 253, 187,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249,\n",
       "       253, 249,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  46, 130,\n",
       "       183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "       229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114,\n",
       "       221, 253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  23,  66,\n",
       "       213, 253, 253, 253, 253, 198,  81,   2,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 171,\n",
       "       219, 253, 253, 253, 253, 195,  80,   9,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55, 172,\n",
       "       226, 253, 253, 253, 253, 244, 133,  11,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       136, 253, 253, 253, 212, 135, 132,  16,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-scale features - aby sme dostali hodnoty 0 - 1 a nie 0 - 255, ako je to teraz\n",
    "def preprocess_data(x, y):\n",
    "    # Normalize images\n",
    "    x = x / 255.0\n",
    "\n",
    "    # One-hot encode labels\n",
    "    y = tf.one_hot(y.astype(np.int32), depth=NR_CLASSES)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    # Load data from files\n",
    "    x_train_all = np.loadtxt(X_TRAIN_PATH, delimiter=',', dtype=np.float32)\n",
    "    y_train_all = np.loadtxt(Y_TRAIN_PATH, delimiter=',', dtype=np.int32)\n",
    "    x_test = np.loadtxt(X_TEST_PATH, delimiter=',', dtype=np.float32)\n",
    "    y_test = np.loadtxt(Y_TEST_PATH, delimiter=',', dtype=np.int32)\n",
    "    \n",
    "    # x -> sú obrázky v numpy\n",
    "    # y -> sú správne čísla na nich\n",
    "\n",
    "    # Validation dataset;\n",
    "    x_val = x_train_all[:VALIDATION_SIZE]\n",
    "    y_val = y_train_all[:VALIDATION_SIZE]\n",
    "\n",
    "    x_train = x_train_all[VALIDATION_SIZE:]\n",
    "    y_train = y_train_all[VALIDATION_SIZE:]\n",
    "\n",
    "    # Preprocess the data\n",
    "    x_train, y_train = preprocess_data(x_train, y_train)\n",
    "    x_val, y_val = preprocess_data(x_val, y_val)\n",
    "    x_test, y_test = preprocess_data(x_test, y_test)\n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tu reteštartovať"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_val, y_val, x_test, y_test = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Setup Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully created directory\n"
     ]
    }
   ],
   "source": [
    "# Folder for Tensorboard\n",
    "folder_name = f'Model 1 at {strftime(\"%m %d %Y - %H %M\")}'\n",
    "directory = os.path.join(LOGGING_PATH, folder_name)\n",
    "\n",
    "try:\n",
    "    os.makedirs(directory)\n",
    "except OSError as exception:\n",
    "    print(exception.strerror)\n",
    "else:\n",
    "    print('Succesfully created directory')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up writers ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writer definition\n",
    "train_writer = create_file_writer(directory + '/train')\n",
    "val_writer = create_file_writer(directory + '/val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "# Model without dropout\n",
    "class HandwritingModel(tf.Module):\n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, output_size):\n",
    "        super(HandwritingModel, self).__init__()\n",
    "        # Initialize weights and biases for the first hidden layer\n",
    "        self.w1 = tf.Variable(tf.random.truncated_normal([input_size, hidden1_size], stddev=0.1))\n",
    "        self.b1 = tf.Variable(tf.constant(0.1, shape=[hidden1_size]))\n",
    "\n",
    "        # Initialize weights and biases for the second hidden layer\n",
    "        self.w2 = tf.Variable(tf.random.truncated_normal([hidden1_size, hidden2_size], stddev=0.1))\n",
    "        self.b2 = tf.Variable(tf.constant(0.1, shape=[hidden2_size]))\n",
    "\n",
    "        # Initialize weights and biases for the output layer\n",
    "        self.w3 = tf.Variable(tf.random.truncated_normal([hidden2_size, output_size], stddev=0.1))\n",
    "        self.b3 = tf.Variable(tf.constant(0.1, shape=[output_size]))\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, TOTAL_INPUTS], dtype=tf.float32)])\n",
    "    def __call__(self, x):\n",
    "        # First hidden layer with ReLU activation\n",
    "        with tf.name_scope('hidden_layer_1'):\n",
    "            z1 = tf.matmul(x, self.w1) + self.b1\n",
    "            a1 = tf.nn.relu(z1)\n",
    "\n",
    "        # Second hidden layer with ReLU activation\n",
    "        with tf.name_scope('hidden_layer_2'):\n",
    "            z2 = tf.matmul(a1, self.w2) + self.b2\n",
    "            a2 = tf.nn.relu(z2)\n",
    "\n",
    "        # Output layer with linear activation\n",
    "        with tf.name_scope('output_layer'):\n",
    "            z3 = tf.matmul(a2, self.w3) + self.b3\n",
    "        return z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = HandwritingModel(TOTAL_INPUTS, n_hidden1, n_hidden2, NR_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Model definition\\n# Model with dropout\\nclass HandwritingModel_drop(tf.Module):\\n    def __init__(self, input_size, hidden1_size, hidden2_size, output_size):\\n        super(HandwritingModel_drop, self).__init__()\\n        # Initialize weights and biases for the first hidden layer\\n        self.w1 = tf.Variable(tf.random.truncated_normal([input_size, hidden1_size], stddev=0.1))\\n        self.b1 = tf.Variable(tf.constant(0.1, shape=[hidden1_size]))\\n\\n        # Initialize weights and biases for the second hidden layer\\n        self.w2 = tf.Variable(tf.random.truncated_normal([hidden1_size, hidden2_size], stddev=0.1))\\n        self.b2 = tf.Variable(tf.constant(0.1, shape=[hidden2_size]))\\n\\n        # Initialize weights and biases for the output layer\\n        self.w3 = tf.Variable(tf.random.truncated_normal([hidden2_size, output_size], stddev=0.1))\\n        self.b3 = tf.Variable(tf.constant(0.1, shape=[output_size]))\\n\\n    @tf.function(input_signature=[tf.TensorSpec(shape=[None, TOTAL_INPUTS], dtype=tf.float32)])\\n    def __call__(self, x):\\n        # First hidden layer with ReLU activation\\n        with tf.name_scope('hidden_layer_1'):\\n            z1 = tf.matmul(x, self.w1) + self.b1\\n            a1 = tf.nn.relu(z1)\\n\\n        # Drop hidden layer\\n        with tf.name_scope('drop_layer'):\\n            dz1 = tf.nn.dropout(a1, rate=0.8, name='dropout_layer')\\n\\n        # Second hidden layer with ReLU activation\\n        with tf.name_scope('hidden_layer_2'):\\n            z2 = tf.matmul(dz1, self.w2) + self.b2\\n            a2 = tf.nn.relu(z2)\\n\\n        # Output layer with linear activation\\n        with tf.name_scope('output_layer'):\\n            z3 = tf.matmul(a2, self.w3) + self.b3\\n        return z3\""
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Model definition\n",
    "# Model with dropout\n",
    "class HandwritingModel_drop(tf.Module):\n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, output_size):\n",
    "        super(HandwritingModel_drop, self).__init__()\n",
    "        # Initialize weights and biases for the first hidden layer\n",
    "        self.w1 = tf.Variable(tf.random.truncated_normal([input_size, hidden1_size], stddev=0.1))\n",
    "        self.b1 = tf.Variable(tf.constant(0.1, shape=[hidden1_size]))\n",
    "\n",
    "        # Initialize weights and biases for the second hidden layer\n",
    "        self.w2 = tf.Variable(tf.random.truncated_normal([hidden1_size, hidden2_size], stddev=0.1))\n",
    "        self.b2 = tf.Variable(tf.constant(0.1, shape=[hidden2_size]))\n",
    "\n",
    "        # Initialize weights and biases for the output layer\n",
    "        self.w3 = tf.Variable(tf.random.truncated_normal([hidden2_size, output_size], stddev=0.1))\n",
    "        self.b3 = tf.Variable(tf.constant(0.1, shape=[output_size]))\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, TOTAL_INPUTS], dtype=tf.float32)])\n",
    "    def __call__(self, x):\n",
    "        # First hidden layer with ReLU activation\n",
    "        with tf.name_scope('hidden_layer_1'):\n",
    "            z1 = tf.matmul(x, self.w1) + self.b1\n",
    "            a1 = tf.nn.relu(z1)\n",
    "\n",
    "        # Drop hidden layer\n",
    "        with tf.name_scope('drop_layer'):\n",
    "            dz1 = tf.nn.dropout(a1, rate=0.8, name='dropout_layer')\n",
    "\n",
    "        # Second hidden layer with ReLU activation\n",
    "        with tf.name_scope('hidden_layer_2'):\n",
    "            z2 = tf.matmul(dz1, self.w2) + self.b2\n",
    "            a2 = tf.nn.relu(z2)\n",
    "\n",
    "        # Output layer with linear activation\n",
    "        with tf.name_scope('output_layer'):\n",
    "            z3 = tf.matmul(a2, self.w3) + self.b3\n",
    "        return z3\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss, Optimisation & Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits, labels):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimiser = tf.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005\n"
     ]
    }
   ],
   "source": [
    "print(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Accuracy Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a accuracy metric\n",
    "def accuracy_fn(predictions, labels):\n",
    "    correct_predictions = tf.equal(tf.argmax(predictions, 1), tf.argmax(labels, 1))\n",
    "    return tf.reduce_mean(tf.cast(correct_predictions, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Input Images in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with tf.name_scope('show_image'):\n",
    "#    x_image = tf.reshape(X, [-1, 28, 28, 1])\n",
    "#    tf.summary.image('image_input', x_image, max_outputs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "# Set-up učiacej slučky\n",
    "size_of_batch = 1000\n",
    "num_examples = y_train.shape[0]\n",
    "print(num_examples)\n",
    "nr_iterations = int(num_examples/size_of_batch)\n",
    "print(nr_iterations)\n",
    "index_in_epoch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Filewriter and Merge Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nakreslenie tf grafu pre učiacu slučku .....\n",
    "def pisac(model, x = \"WDO\"):\n",
    "    # Get the concrete function\n",
    "    concrete_function = model.__call__.get_concrete_function()\n",
    "\n",
    "    # Create a summary file writer\n",
    "    if x == \"DO\":\n",
    "        writer = tf.summary.create_file_writer(directoryDO)\n",
    "    else:\n",
    "        writer = tf.summary.create_file_writer(directory)\n",
    "\n",
    "    # Use the writer to write the graph\n",
    "    with writer.as_default():\n",
    "        tf.summary.graph(concrete_function.graph)\n",
    "        writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcia vráti batch z dát......\n",
    "def get_batches(batch_size, x, y):\n",
    "    for start in range(0, len(x), batch_size):\n",
    "        end = start + batch_size\n",
    "        yield x[start:end], y[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop - Funkcia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nauc (model):\n",
    "    for epoch in range(nr_epochs):\n",
    "        total_accuracy = tf.constant(0, dtype=tf.float32)\n",
    "        num_batches = tf.constant(0, dtype=tf.float32)\n",
    "\n",
    "        for batch_x, batch_y in get_batches(size_of_batch, x_train, y_train):\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Vypočítame predpoved\n",
    "                logits = model(batch_x)\n",
    "                # Stratu\n",
    "                loss = loss_fn(logits, batch_y)\n",
    "                # úspešnosť\n",
    "                accuracy = accuracy_fn(logits, batch_y)\n",
    "                total_accuracy += accuracy\n",
    "\n",
    "        # Compute gradients\n",
    "        gradients = tape.gradient(loss, [model.w1, model.b1, model.w2, model.b2, model.w3, model.b3])\n",
    "        if any(g is None for g in gradients):\n",
    "            print(gradients)\n",
    "            raise ValueError(\"One or more gradients are None\")\n",
    "            \n",
    "        # Apply gradients\n",
    "        optimiser.apply_gradients(zip(gradients, [model.w1, model.b1, model.w2, model.b2, model.w3, model.b3]))\n",
    "        \n",
    "        # Average accuracy\n",
    "        average_train_accuracy = total_accuracy / num_batches\n",
    "            \n",
    "        with train_writer.as_default():\n",
    "            for var, varname, grad in zip([model.w1, model.b1, model.w2, model.b2, model.w3, model.b3], ['w1', 'b1', 'w2', 'b2', 'w3', 'b3'], gradients):\n",
    "                tf.summary.histogram(varname, var, step=epoch)\n",
    "                tf.summary.histogram(f\"{varname}/grad\", grad, step=epoch)\n",
    "            # Performance metrics\n",
    "            with tf.name_scope('performance'):\n",
    "                tf.summary.scalar('accuracy', accuracy, step=epoch)\n",
    "                tf.summary.scalar('cost', loss, step=epoch)\n",
    "\n",
    "        num_batches += 1\n",
    "\n",
    "        # Validation\n",
    "        val_logits = model(tf.cast(x_val, tf.float32))\n",
    "        val_loss = loss_fn(val_logits, y_val)\n",
    "        val_accuracy = accuracy_fn(val_logits, y_val)\n",
    "        \n",
    "        with val_writer.as_default():\n",
    "            with tf.name_scope('performance'):\n",
    "                tf.summary.scalar('accuracy', val_accuracy, step=epoch)\n",
    "                tf.summary.scalar('cost', val_loss, step=epoch)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}, Loss (validation): {val_loss.numpy()}, Training Accuracy: {average_train_accuracy.numpy() * 100:.20f}%, Validation Accuracy: {val_accuracy.numpy() * 100:.2f}%\")\n",
    "\n",
    "        train_writer.flush()\n",
    "        val_writer.flush()\n",
    "\n",
    "    # Napíše graf modelu......\n",
    "    pisac(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A teraz natrenovanie..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss (validation): 2.4630229473114014, Training Accuracy: inf%, Validation Accuracy: 5.94%\n",
      "Epoch 2, Loss (validation): 2.3123090267181396, Training Accuracy: inf%, Validation Accuracy: 12.26%\n",
      "Epoch 3, Loss (validation): 2.1967904567718506, Training Accuracy: inf%, Validation Accuracy: 20.56%\n",
      "Epoch 4, Loss (validation): 2.1015095710754395, Training Accuracy: inf%, Validation Accuracy: 28.01%\n",
      "Epoch 5, Loss (validation): 2.018244504928589, Training Accuracy: inf%, Validation Accuracy: 35.70%\n",
      "Epoch 6, Loss (validation): 1.9416017532348633, Training Accuracy: inf%, Validation Accuracy: 42.06%\n",
      "Epoch 7, Loss (validation): 1.8679312467575073, Training Accuracy: inf%, Validation Accuracy: 47.54%\n",
      "Epoch 8, Loss (validation): 1.7950366735458374, Training Accuracy: inf%, Validation Accuracy: 51.95%\n",
      "Epoch 9, Loss (validation): 1.721903920173645, Training Accuracy: inf%, Validation Accuracy: 55.74%\n",
      "Epoch 10, Loss (validation): 1.6482328176498413, Training Accuracy: inf%, Validation Accuracy: 58.92%\n",
      "Epoch 11, Loss (validation): 1.57466721534729, Training Accuracy: inf%, Validation Accuracy: 61.46%\n",
      "Epoch 12, Loss (validation): 1.502266764640808, Training Accuracy: inf%, Validation Accuracy: 63.77%\n",
      "Epoch 13, Loss (validation): 1.4321407079696655, Training Accuracy: inf%, Validation Accuracy: 65.66%\n",
      "Epoch 14, Loss (validation): 1.3654377460479736, Training Accuracy: inf%, Validation Accuracy: 67.16%\n",
      "Epoch 15, Loss (validation): 1.3028210401535034, Training Accuracy: inf%, Validation Accuracy: 68.37%\n",
      "Epoch 16, Loss (validation): 1.24454927444458, Training Accuracy: inf%, Validation Accuracy: 69.35%\n",
      "Epoch 17, Loss (validation): 1.1903042793273926, Training Accuracy: inf%, Validation Accuracy: 70.16%\n",
      "Epoch 18, Loss (validation): 1.1396065950393677, Training Accuracy: inf%, Validation Accuracy: 70.93%\n",
      "Epoch 19, Loss (validation): 1.092010498046875, Training Accuracy: inf%, Validation Accuracy: 71.62%\n",
      "Epoch 20, Loss (validation): 1.047179102897644, Training Accuracy: inf%, Validation Accuracy: 72.46%\n",
      "Epoch 21, Loss (validation): 1.004977822303772, Training Accuracy: inf%, Validation Accuracy: 73.55%\n",
      "Epoch 22, Loss (validation): 0.9653103351593018, Training Accuracy: inf%, Validation Accuracy: 74.33%\n",
      "Epoch 23, Loss (validation): 0.928337574005127, Training Accuracy: inf%, Validation Accuracy: 74.90%\n",
      "Epoch 24, Loss (validation): 0.8942240476608276, Training Accuracy: inf%, Validation Accuracy: 75.62%\n",
      "Epoch 25, Loss (validation): 0.863024115562439, Training Accuracy: inf%, Validation Accuracy: 76.15%\n",
      "Epoch 26, Loss (validation): 0.8347963094711304, Training Accuracy: inf%, Validation Accuracy: 76.55%\n",
      "Epoch 27, Loss (validation): 0.8094208240509033, Training Accuracy: inf%, Validation Accuracy: 77.01%\n",
      "Epoch 28, Loss (validation): 0.7866454124450684, Training Accuracy: inf%, Validation Accuracy: 77.29%\n",
      "Epoch 29, Loss (validation): 0.766194760799408, Training Accuracy: inf%, Validation Accuracy: 77.52%\n",
      "Epoch 30, Loss (validation): 0.7477968335151672, Training Accuracy: inf%, Validation Accuracy: 77.91%\n",
      "Epoch 31, Loss (validation): 0.7312039136886597, Training Accuracy: inf%, Validation Accuracy: 78.09%\n",
      "Epoch 32, Loss (validation): 0.7163384556770325, Training Accuracy: inf%, Validation Accuracy: 78.50%\n",
      "Epoch 33, Loss (validation): 0.7031092643737793, Training Accuracy: inf%, Validation Accuracy: 78.71%\n",
      "Epoch 34, Loss (validation): 0.6914057731628418, Training Accuracy: inf%, Validation Accuracy: 78.87%\n",
      "Epoch 35, Loss (validation): 0.6810523271560669, Training Accuracy: inf%, Validation Accuracy: 79.10%\n",
      "Epoch 36, Loss (validation): 0.6718248128890991, Training Accuracy: inf%, Validation Accuracy: 79.23%\n",
      "Epoch 37, Loss (validation): 0.6635624766349792, Training Accuracy: inf%, Validation Accuracy: 79.35%\n",
      "Epoch 38, Loss (validation): 0.6560554504394531, Training Accuracy: inf%, Validation Accuracy: 79.53%\n",
      "Epoch 39, Loss (validation): 0.6491451263427734, Training Accuracy: inf%, Validation Accuracy: 79.66%\n",
      "Epoch 40, Loss (validation): 0.6427023410797119, Training Accuracy: inf%, Validation Accuracy: 79.80%\n",
      "Epoch 41, Loss (validation): 0.6366249918937683, Training Accuracy: inf%, Validation Accuracy: 79.95%\n",
      "Epoch 42, Loss (validation): 0.6308535933494568, Training Accuracy: inf%, Validation Accuracy: 80.22%\n",
      "Epoch 43, Loss (validation): 0.6253600120544434, Training Accuracy: inf%, Validation Accuracy: 80.46%\n",
      "Epoch 44, Loss (validation): 0.6201123595237732, Training Accuracy: inf%, Validation Accuracy: 80.60%\n",
      "Epoch 45, Loss (validation): 0.6151440739631653, Training Accuracy: inf%, Validation Accuracy: 80.68%\n",
      "Epoch 46, Loss (validation): 0.6104526519775391, Training Accuracy: inf%, Validation Accuracy: 80.86%\n",
      "Epoch 47, Loss (validation): 0.606041431427002, Training Accuracy: inf%, Validation Accuracy: 81.02%\n",
      "Epoch 48, Loss (validation): 0.6019338965415955, Training Accuracy: inf%, Validation Accuracy: 81.26%\n",
      "Epoch 49, Loss (validation): 0.5981324911117554, Training Accuracy: inf%, Validation Accuracy: 81.40%\n",
      "Epoch 50, Loss (validation): 0.5945852398872375, Training Accuracy: inf%, Validation Accuracy: 81.58%\n",
      "Epoch 51, Loss (validation): 0.5912322402000427, Training Accuracy: inf%, Validation Accuracy: 81.66%\n",
      "Epoch 52, Loss (validation): 0.5879663228988647, Training Accuracy: inf%, Validation Accuracy: 81.81%\n",
      "Epoch 53, Loss (validation): 0.5847528576850891, Training Accuracy: inf%, Validation Accuracy: 81.98%\n",
      "Epoch 54, Loss (validation): 0.5815886855125427, Training Accuracy: inf%, Validation Accuracy: 82.10%\n",
      "Epoch 55, Loss (validation): 0.5785024166107178, Training Accuracy: inf%, Validation Accuracy: 82.27%\n",
      "Epoch 56, Loss (validation): 0.5755500793457031, Training Accuracy: inf%, Validation Accuracy: 82.35%\n",
      "Epoch 57, Loss (validation): 0.5728129744529724, Training Accuracy: inf%, Validation Accuracy: 82.44%\n",
      "Epoch 58, Loss (validation): 0.570331335067749, Training Accuracy: inf%, Validation Accuracy: 82.51%\n",
      "Epoch 59, Loss (validation): 0.5681678652763367, Training Accuracy: inf%, Validation Accuracy: 82.58%\n",
      "Epoch 60, Loss (validation): 0.56634920835495, Training Accuracy: inf%, Validation Accuracy: 82.57%\n",
      "Epoch 61, Loss (validation): 0.5648635625839233, Training Accuracy: inf%, Validation Accuracy: 82.61%\n",
      "Epoch 62, Loss (validation): 0.5636937618255615, Training Accuracy: inf%, Validation Accuracy: 82.67%\n",
      "Epoch 63, Loss (validation): 0.562784731388092, Training Accuracy: inf%, Validation Accuracy: 82.68%\n",
      "Epoch 64, Loss (validation): 0.5620842576026917, Training Accuracy: inf%, Validation Accuracy: 82.72%\n",
      "Epoch 65, Loss (validation): 0.5615243911743164, Training Accuracy: inf%, Validation Accuracy: 82.71%\n",
      "Epoch 66, Loss (validation): 0.5610451102256775, Training Accuracy: inf%, Validation Accuracy: 82.73%\n",
      "Epoch 67, Loss (validation): 0.560590922832489, Training Accuracy: inf%, Validation Accuracy: 82.80%\n",
      "Epoch 68, Loss (validation): 0.5601103901863098, Training Accuracy: inf%, Validation Accuracy: 82.83%\n",
      "Epoch 69, Loss (validation): 0.559561550617218, Training Accuracy: inf%, Validation Accuracy: 82.88%\n",
      "Epoch 70, Loss (validation): 0.5589213371276855, Training Accuracy: inf%, Validation Accuracy: 82.92%\n"
     ]
    }
   ],
   "source": [
    "# Model bez Dropoff\n",
    "nauc(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAYAAAByDd+UAAABIUlEQVR4Ae2U3Q2EIAzH6+XeHUFXcAI/tvJJ3UQ3cAKjmziCTsBdTZocaqFiwsPFJgZLgV/5Aw3U18CjvTyyNtQDPFW8aRrAz8UC6aUhwDRNGmcYBs23OaIzRBiB0jQFhBCIErGBKP6mH65d1xXGcYS+7yEMQ20YwikRLWByUFKbzfPMDsnzXNV1zcb3AZGkURSxOeMur5gIeGVB21jvQNhrfMVflkVlWaawlZr1lnIS0VNJkuRwe7k52O8ELIpiWxMvTFVVpvWPMakUOK4sy01CbF1NtEOSD9Nt2xZMz+S4Jb3HCPwFOcmnszaPLd4I67oO4jgG6eOWnCcLxHQQKjWqqVTU2Xmuh382T1JTjTtks7wR8F7aHuCN0zqf+v+SfgCAJwNqfagb/wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=28x28>"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open('MNIST/test_img.png')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konverzia do ČB obrázka\n",
    "bw = img.convert('L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = np.invert(bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = img_array.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the test_img to match the expected shape (None, 784)\n",
    "test_img = test_img.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tf.argmax(mod(test_img), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for test image is [2]\n"
     ]
    }
   ],
   "source": [
    "print(f'Prediction for test image is {predictions}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teraz všetky obrázky....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_directory = 'MNIST/'\n",
    "processed_images = []\n",
    "preds = []\n",
    "\n",
    "def tfprocimg(obr):\n",
    "    img = Image.open(obr)\n",
    "    bw = img.convert('L')\n",
    "    img_array = np.invert(bw)\n",
    "    test_img = img_array.ravel()\n",
    "    test_img = test_img.reshape(1, -1)\n",
    "    return test_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: MNIST/test_img.png is number [2]\n",
      "File: MNIST/test_img1.png is number [9]\n",
      "File: MNIST/test_img1a.png is number [0]\n",
      "File: MNIST/test_img6.png is number [6]\n",
      "File: MNIST/test_img7.png is number [2]\n",
      "File: MNIST/test_img7a.png is number [1]\n",
      "File: MNIST/test_img7b.png is number [1]\n",
      "File: MNIST/test_img9.png is number [3]\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(image_directory):\n",
    "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "        # Check if the file is a JPEG or PNG image (you can add more formats as needed)\n",
    "        file_path = os.path.join(image_directory, filename)\n",
    "        \n",
    "        try:\n",
    "            # Open and process the image using Pillow\n",
    "            ts_im = tfprocimg(file_path)\n",
    "            predict = tf.argmax(mod(ts_im), axis=1)\n",
    "            print(f'File: {file_path} is number {predict}')\n",
    "            \n",
    "            # Append the processed image to the list and preds\n",
    "            processed_images.append(file_path)\n",
    "            preds.append(predict)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge:** Calculate the accuracy over the test dataset (```x_test``` and ```y_test```). Use your knowledge of running a session to get the accuracy. Display the accuracy as a percentage rounded to two decimal numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.0%\n",
      "Accuracy on test set is 83.17%\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = mod(x_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(predictions, axis=1), tf.argmax(y_test, axis=1)), tf.float32))\n",
    "\n",
    "# Convert accuracy to a percentage and round to two decimal places\n",
    "accuracy_percent = tf.round(test_accuracy * 100, 2)\n",
    "\n",
    "# Print the accuracy as a percentage\n",
    "print(f\"Accuracy: {accuracy_percent}%\")\n",
    "                          \n",
    "print(f'Accuracy on test set is {test_accuracy:0.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Reset for the Next Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer.close()\n",
    "val_writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
